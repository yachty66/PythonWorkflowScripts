{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "### What is the wikipedia link to the algorithm?\n",
    "\n",
    "- [Link](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "\n",
    "### Which type of machine learning algorithm is this?\n",
    "\n",
    "- Supervised learning\n",
    "\n",
    "### What is the best video tutorial on this algorithm?\n",
    "\n",
    "- [Video](https://www.youtube.com/watch?v=yIYKR4sgzI8)\n",
    "\n",
    "### What is the best text?\n",
    "\n",
    "- [Link](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html)\n",
    "\n",
    "### What is the best picture which describes the algorithm?\n",
    "\n",
    "- ![Linear Regression](Images/BinaryLogisticRegression.png)\n",
    "\n",
    "### What is one case for which the algorithm is used for?\n",
    "\n",
    "- Determining the probability of a patient developing a particular disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data on which the algorithm gets proofed on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From scratch implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for reproducing the algorithm:\n",
    "1. **Hypothesis**\n",
    "    - Sigmoid function outputs 0 or 1 based on a set treshold with formula from linear regression plugged in.\n",
    "2. **Cost function** \n",
    "    - Cross entropy which is divided into two fucntions - one for $y = 0$ and for $y = 1$\n",
    "3. **Gradient descent**\n",
    "    - Using the partial derivative of the sigmoid hypothesis function\n",
    "4. **Logistic regression**\n",
    "    - Putting everything into one model\n",
    "5. **Validation**\n",
    "    - Comparison with scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLogistic regression:\\ngetting input \\n\\nSame like multiple linear regression but this time i output 0 or 1 with my hypothesis\\nThe cost function and with that the gradient looks different but from the logic its the same \\n\\n'"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Logistic regression:\n",
    "getting input \n",
    "\n",
    "Same like multiple linear regression but this time i output 0 or 1 with my hypothesis\n",
    "The cost function and with that the gradient looks different but from the logic its the same \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Iris dataset contains three classes (three different types of the iris flower) and four features. Given an input of four features from an Iris flower I need to use logistic regression for outputting the respective class.\n",
    "\n",
    "**Dataset**: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "xTest = X[130:] \n",
    "yTest = y[130:] \n",
    "X = X[:130]\n",
    "y = y[:130]\n",
    "thetas = np.ones(X[0].size)\n",
    "learningRate = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S(z)= \\frac{1}{1+\\exp^{-z}}$ for binary and $\\sigma(z_i)= \\frac{e^{z_{(i)}}}{\\sum^{K}_{j=1}e^{z_{(j)}}}$ for multi class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in my thinking i have to implement the decision boundary with the hypothesis somehow if i have three classes than class one is <= 0.33 class 2 <= 0.66 etc\n",
    "#need to check if this is correct. check in sheets how decision boundary works exactly\n",
    "#can i not just get the max value from my predictions. I mean first entry corresponds to first class etc always\n",
    "\n",
    "\"\"\"\n",
    "In words: we apply the standard exponential function to each element ð‘§ð‘– of the input vector ð‘§ and normalize these values by dividing by the sum of all these\n",
    "exponentials; this normalization ensures that the sum of the components of the output vector Ïƒ(ð‘§) is 1\n",
    "\"\"\"\n",
    "\n",
    "def hypothesis2(x):\n",
    "    \"\"\"Function for converting input values into probabilites.\n",
    "\n",
    "    Args:\n",
    "        z (list): input vector\n",
    "\n",
    "    Returns:\n",
    "        np.array: probabilities for every element from the input vector\n",
    "    \"\"\"\n",
    "    l=[]\n",
    "    for i in range(len(x)):\n",
    "        softmax = np.exp(x[i]) / sum(np.exp(x))\n",
    "        l.append(softmax)\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hypothesis(x, thetas):\n",
    "    \"\"\"Function for converting input values into probabilites.\n",
    "\n",
    "    Args:\n",
    "        z (list): input vector\n",
    "\n",
    "    Returns:\n",
    "        np.array: probabilities for every element from the input vector\n",
    "    \"\"\"\n",
    "    l=[]\n",
    "    for i in range(len(x)):\n",
    "        softmax = np.exp(x[i]) / sum(np.exp(np.dot(X, thetas)))\n",
    "        l.append(softmax)\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81032902, 0.16360261, 0.02003419, 0.00603418])"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis2(X[0])\n",
    "#problem is that this result does not represent the three classes what it normally should do\n",
    "#how do i know many classes exist?\n",
    "#i need to count the number of numbers which are different from each other and than range through this number? yes and also "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction \n",
    "\n",
    "Returns highest probability from hypothesis with respective class identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(probabilities):\n",
    "    \"\"\"On a given array of probabilities class with highest probabilitie value is returned.\n",
    "\n",
    "    Args:\n",
    "        probabilities (np.array): Array of probabilities\n",
    "\n",
    "    Returns:\n",
    "        int: Class\n",
    "    \"\"\"\n",
    "    highestValueClass = np.where(probabilities == np.max(probabilities))[0][0]\n",
    "    return highestValueClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta)=-\\frac{1}{m} \\sum^{m}_{i=1}[y^{(i)}\\log(h_{\\theta}(x^{(i)}))+(1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(x, y, theta):\n",
    "    \"\"\"Shows differences between predicted and actual y value.\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Vector of probabilities\n",
    "        y (int): Class prediction\n",
    "\n",
    "    Returns:\n",
    "        int: Error between actual and predicted y.\n",
    "    \"\"\"\n",
    "    j = y * np.log(hypothesis(x, theta)) + (1 - y) * np.log(1 - hypothesis(x, theta))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "$\\theta_j \\leftarrow \\theta_j - \\alpha \\frac{1}{m} \\sum^{m}_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j$\n",
    "\n",
    "$f'(\\theta_1) = -x_1(y - (\\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n)) \\\\$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTheta(x, y, thetas, learningRate):\n",
    "    \"\"\"Updates weights for finding optimum. https://www.baeldung.com/cs/gradient-descent-logistic-regression\n",
    "    Args:\n",
    "        x (np.array): Vector of probabilities\n",
    "        y (int): Class prediction\n",
    "        theta (np.array): Weights which get updated\n",
    "        learningRate (float): Determines step size\n",
    "\n",
    "    Returns:\n",
    "        np.array: Updated weights\n",
    "    \"\"\"\n",
    "    thetaList = []\n",
    "\n",
    "    for i in range(len(thetas)):\n",
    "        #x = X[i] mÃ¼sste ein wert sein ist aber ganzes array y = y[i]\n",
    "        newTheta = thetas[i] - learningRate*(-x[i]*(y-(np.dot(x,thetas))))\n",
    "        #thetaNew = thetas[i] - learningRate*(-X[i]*(y-(np.dot(X,thetas))))\n",
    "        #newTheta = thetas[i] - learningRate * 1/len(x) * sum((hypothesis(x)-y) * x)\n",
    "        #thetaNew = thetas[i] - learningRate*(-X[i]*(y-(np.dot(X,thetas))))\n",
    "        thetaList.append(newTheta)\n",
    "    return thetaList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i have confusion torwards how i have to implement my decision boundary \n",
    "def logisticRegression(X, y, theta, learningRate, iterations):\n",
    "    resultsCostFunction = []\n",
    "    resultsTheta = []\n",
    "    #resultPrediction\n",
    "    for i in range(iterations):\n",
    "        #is there one bias term which i use constant or does this value as often as x differently\n",
    "        #i have two partial derivatives for weight and bias\n",
    "        #i have only given one gradient descent equation - which one is the right one now? \n",
    "        for i in range(len(X)):\n",
    "            j = costFunction(X[i], y[i], theta)\n",
    "            theta = updateTheta(X[i], y[i], theta, learningRate)\n",
    "            resultsCostFunction.append(j)\n",
    "            resultsTheta.append(theta)\n",
    "    return resultsCostFunction, resultsTheta\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r9/30wn6mvs2md_xllcvl5lpn0m0000gn/T/ipykernel_47253/3848176310.py:11: RuntimeWarning: invalid value encountered in log\n",
      "  j = y * np.log(hypothesis(x, theta)) + (1 - y) * np.log(1 - hypothesis(x, theta))\n"
     ]
    }
   ],
   "source": [
    "costFunctionResults, thetasResults = logisticRegression(X, y, thetas, learningRate, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting dependent variables with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(X, thetas):\n",
    "    yPred = []\n",
    "    for i in range(len(X)):\n",
    "        y = prediction(hypothesis(X[i], thetas))\n",
    "        yPred.append(y)\n",
    "    print(yPred)\n",
    "    return np.array(yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.037586007019736335, -0.09810514835691256, 0.23112464993204357, 0.4248515844717129]\n"
     ]
    }
   ],
   "source": [
    "print(thetasResults[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.hypothesis(x, thetas)>"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. check if hypothesis is alright (call function and input xTest should return always 3 prob)\n",
    "    - now i call it like \n",
    "    - \n",
    "2. compare hypothesis with xTest (do i get right results?)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val(xTest, thetasResults[-1])\n",
    "#get wrong results because my theta is lacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logisticRegression = LogisticRegression().fit(X, y)\n",
    "\n",
    "resultsScikitLearn = logisticRegression.predict(xTest)\n",
    "print(resultsScikitLearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> really code explanation https://mlcorner.com/multiple-logistic-regression-explained-for-machine-learning/ I should save it \n",
    "\n",
    "1. Go again from top to to button through every function and fix code\n",
    "\n",
    "1. Pick dataset\n",
    "    - Iris from example page https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "2. Understand what I need to do for a logistic regression with this dataset\n",
    "    1. load data and play around to understand\n",
    "    2. Put data creation section on top because this helps a lot for understanding the problem\n",
    "    3. Link to description of dataset \n",
    "    4. Own description of the task which needs to be accomplished\n",
    "3. Make a plan\n",
    "    1. hypothesis\n",
    "        - instead of sigma implementing softmax?\n",
    "        1. Implementing softmax\n",
    "        2. Adding docstrings \n",
    "    2. decision boundary\n",
    "    3. cost\n",
    "    4. gradient\n",
    "    5. logistic regression\n",
    "    6. validation\n",
    "\n",
    "\n",
    "### Starting new and understanding the whole workflow \n",
    "\n",
    "1. Z = product of my weights and the input features\n",
    "2. softmax with Z in counter "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
